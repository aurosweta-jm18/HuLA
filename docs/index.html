<!doctype html>
<html lang="en">

<head>
  <meta name="google-site-verification" content="ftFOlJETX-2KNjaPh8W6s8lhigItRuu9fOmjHZZ0nY0" />
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <!-- Bootstrap CSS -->
  <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/css/bootstrap.min.css"
    integrity="sha384-MCw98/SFnGE8fJT3GXwEOngsV7Zt27NXFoaoApmYm81iuXoPkFOJwJ8ERdknLPMO" crossorigin="anonymous">

  <title>HuLA</title>

  <style type="text/css">
    table {
      width: 100%;
      table-layout: fixed;
    }

    body,
    p,
    div,
    figcaption {
      font-size: 1rem;
      color: #000000;
    }

	.section-title {
	  font-size: 1.8rem;
	  text-align: center;
	  font-weight: bold;
	  position: relative;
	  padding: 5px 10px;
	  color: navy;   /* make text navy blue */
	}
	
	.section-title::after {
	  content: "";
	  display: block;
	  width: 100%;
	  border-bottom: 2px solid navy; /* underline also navy */
	  margin-top: 5px;
	}

    audio {
      width: 100%;
    }

    .table th,
    .table td {
      border-left: none !important;
      border-right: none !important;
    }

    .table {
      border: none;
    }

    thead>tr>th:first-child {
      width: 200px;
    }

    @media (max-width: 767px) {
      .big-screen {
        display: none;
      }
    }

    @media (min-width: 767px) {
      .small-screen {
        display: none;
      }
    }
  </style>
</head>

<body>
  <header class="header">
    <div class="jumbotron text-center" style="background-color: #001f3f;">
      <div class="container">
        <h1 class="text-light">HuLA: Prosody-Aware Anti-Spoofing with Multi-task Learning for
          Expressive and Emotional Speech
        </h1>
        <p style="font-size: 1rem; color: lightgray; font-style: italic;">
          (This work is submitted to IEEE Transactions on Affective Computing)
        </p>
		<p style="font-size: 1.25rem; color:#FFFFFF;">
		  Aurosweta Mahapatra, Ismail Rasim Ulgen, Berrak Sisman
		  <br>
		  <span style="font-size:1rem;">
		    amahapa2@jhu.edu &nbsp; iulgen1@jhu.edu &nbsp; sisman@jhu.edu
		  </span>
		</p>
		
		<p style="font-size: 1rem; color:#FFFFFF;">
		  Electrical and Computer Engineering Department, Johns Hopkins University, USA
		</p>
		
		<p style="font-size: 1.2rem; color: #FFFFFF; font-weight: bold;">
		  <a href="https://github.com/aurosweta-jm18/HuLA" target="_blank" style="color:#B0E0E6;">
		    Codes and Pre-trained Models (Coming Soon)
		  </a>
		</p>
		<p style="font-size: 1.2rem; color: #FFFFFF; font-weight: bold;">
		  <a href="https://www.arxiv.org/abs/2509.21676" target="_blank" style="color:#B0E0E6;">
		    Paper Link
		  </a>
		</p>

      </div>
    </div>
  </header>

  <main>
    <div class="container">
      <div class="row" id="result">
        <div class="col-md-12">
          <figcaption style="text-align: justify; font-size: 1rem; color: #000000;">
            <b>Abstract</b>: Current anti-spoofing systems remain vulnerable to expressive and emotional synthetic
            speech, since they rarely leverage prosody as a discriminative cue. Prosody is central to human
            expressiveness and emotion, and humans instinctively use prosodic cues such as <i>F<sub>0</sub></i> contours
            and voiced/unvoiced structure to distinguish natural from synthetic speech. In this paper, we propose
            <i>HuLA</i>, a two-stage prosody-aware multi-task learning framework for spoof detection. In Stage 1, a
            self-supervised learning (SSL) backbone is trained on real speech with auxiliary tasks of
            <i>F<sub>0</sub></i> prediction and voiced/unvoiced classification, enhancing its ability to capture natural
            prosodic variation. In Stage 2, the model is jointly optimized for spoof detection and prosody tasks on both
            real and synthetic data, leveraging prosodic awareness to detect mismatches between natural and expressive
            synthetic speech. Experiments show that HuLA consistently outperforms strong baselines on challenging
            out-of-domain datasets, including expressive, emotional, and cross-lingual attacks.
			<br /><br />
			<h2 class="section-title">Proposed Idea: Listen Like a Human</h2>
			<p>
			  Our goal was to design a model to listen like a human by understanding the
			  prosodic variation from real speech and use that knowledge to build up
			  understanding about prosodic variation in real and fake speech while also
			  capturing other spoof-relevant cues.
			</p>
			
			<img src="./HuLA_proposed_startergy.png" height="350" alt="Overview of Proposed Idea"
			  style="display: block; margin: 0 auto;" />
			
			<p style="text-align: center; font-size: 1rem; color: #000000;">
			  Figure: Stage 1 models the natural prosodic variation of real speech, while
			  Stage 2 discriminates between real and synthetic expressive speech.
			</p>
			
			<h2 class="section-title">HuLA</h2>
			
			<!-- Side by side training phase figures -->
			<div style="display: flex; justify-content: center; align-items: flex-start; gap: 20px; margin-bottom: 20px; flex-wrap: wrap;">
			  <figure style="text-align: center; margin: 0;">
			    <img src="./HuLA_MTL1.png" height="350" alt="Training phase of HuLA" />
			  </figure>
			
			  <figure style="text-align: center; margin: 0;">
			    <img src="./HuLA_MTL2.png" height="350" alt="Training phase continuation of HuLA" />
			  </figure>
			</div>
			
			<!-- Combined caption -->
			<p style="text-align: center; font-size: 1rem; color: #000000; margin-top: -10px;">
			  Figure: Training phase of HuLA, the proposed prosody-aware multi-task learning method for anti-spoofing.
			  Blue blocks indicate initialization from pretrained models, while pink blocks represent training from scratch.
			</p>
			
			<!-- Inference figure -->
			<figure style="text-align: center; margin-top: 30px;">
			  <img src="./HuLA_inf.png" height="200" alt="Inference phase of HuLA" />
			  <figcaption style="font-size: 1rem; color: #000000; margin-top: 8px;">
			    Figure: Inference phase of HuLA.
			  </figcaption>
			</figure>



		<h2 class="section-title">Datasets Used for Evaluation</h2>
		<ul>
		  <li><a href="https://datashare.ed.ac.uk/handle/10283/3336" target="_blank">ASVspoof 2019</a></li>
		  <li><a href="https://www.asvspoof.org/index2021.html" target="_blank">ASVspoof 2021</a></li>
		  <li><a href="https://zenodo.org/records/14498691" target="_blank">ASVspoof 2024</a></li>
		  <li><a href="https://zenodo.org/records/10443769" target="_blank">EmoFake</a></li>
		  <li><a href="https://ieeexplore.ieee.org/abstract/document/10003644" target="_blank">Mixed Emotions</a></li>
		  <li><a href="https://zenodo.org/records/7370805" target="_blank">HABLA</a></li>
		  <li><a href="https://zenodo.org/records/10843991" target="_blank">ADD 2022</a></li>
		</ul>
			  
		<h2 class="section-title">Results and Discussion</h2>
		<!-- Tables displayed side by side -->
		<div style="display: flex; justify-content: center; align-items: flex-start; gap: 20px; flex-wrap: wrap; margin-bottom: 20px;">
		  <figure style="margin: 0; text-align: center;">
		    <img src="./Table_IV.png" alt="ASVspoof Results" style="width: 350px; max-width: 100%; height: auto;" />
		    <figcaption style="font-size: 0.9rem; margin-top: 8px;">ASVspoof Results</figcaption>
		  </figure>
		
		  <figure style="margin: 0; text-align: center;">
		    <img src="./Table_V.png" alt="Emotional Datasets Results" style="width: 350px; max-width: 100%; height: auto;" />
		    <figcaption style="font-size: 0.9rem; margin-top: 8px;">Emotional Datasets Results</figcaption>
		  </figure>
		
		  <figure style="margin: 0; text-align: center;">
		    <img src="./Table_VI.png" alt="Non-English Datasets Results" style="width: 350px; max-width: 100%; height: auto;" />
		    <figcaption style="font-size: 0.9rem; margin-top: 8px;">Non-English Datasets Results</figcaption>
		  </figure>
		</div>
		
		<!-- Explanatory write-up -->
		<p style="font-size: 1rem; line-height: 1.6; text-align: justify; max-width: 900px; margin: 0 auto;">
		  Our experiments demonstrate the effectiveness of HuLA, a two-stage MTL framework that improves spoof detection
		  through explicit prosody modeling. Although trained only on ASVspoof 2019, which lacks the diversity and realism of
		  recent attacks, HuLA generalizes well across datasets that differ substantially from the training domain. Several of these
		  sets include expressive and emotional synthetic speech, which typically fool state-of-the-art baselines. HuLA benefits
		  from prosody-aware training, which equips the model to detect mismatches in expressiveness that are not dataset-dependent.
		  This aligns with our design principle of listening like a human: just as listeners use prosodic cues to judge naturalness,
		  HuLA leverages prosodic variation in both real and spoofed speech to capture subtle differences in expressiveness.
		</p>


          <h3 style="color:#001f3f;">References</h3>
          <ol>
            <li>Wang, Xin, et al. "ASVspoof 2019: A large-scale public database of synthesized, converted and replayed
              speech." <i>Computer Speech & Language</i> 64 (2020): 101114.</li>
            <li>Liu, Xuechen, et al. "ASVspoof 2021: Towards spoofed and deepfake speech detection in the wild."
              <i>IEEE/ACM Transactions on Audio, Speech, and Language Processing</i> 31 (2023): 2507-2522.</li>
            <li>Wang, Xin, et al. "ASVspoof 5: Crowdsourced speech data, deepfakes, and adversarial attacks at scale."
              <i>arXiv preprint</i> arXiv:2408.08739 (2024).</li>
            <li>Zhao, Yan, et al. "Emofake: An initial dataset for emotion fake audio detection." <i>China National
                Conference on Chinese Computational Linguistics</i>. Springer Nature Singapore, 2024.</li>
            <li>Zhou, Kun, et al. "Speech synthesis with mixed emotions." <i>IEEE Transactions on Affective Computing</i>
              14.4 (2022): 3120-3134.</li>
            <li>Flórez, PA Tamayo, Rubén Manrique, and B. Pereira Nunes. "HABLA: A dataset of Latin American Spanish
              accents for voice anti-spoofing." <i>Proc. Interspeech</i>. Vol. 2023.</li>
            <li>Yi, Jiangyan, et al. "ADD 2022: the first audio deep synthesis detection challenge." <i>ICASSP 2022</i>.
            </li>
          </ol>
        </div>
      </div>
    </div>
  </main>

  <footer class="bg-dark text-light mt-4 pt-3 pb-2">
    <div class="container">
      <p class="text-center mb-0">&copy; 2025 HuLA Project Page</p>
    </div>
  </footer>

  <!-- JS Scripts -->
  <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js"
    integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo"
    crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.3/umd/popper.min.js"
    integrity="sha384-ZMP7rVo3mIykV+2+9J3UJ46jBk0WLaUAdn689aCwoqbBJiSnjAK/l8WvCWPIPm49"
    crossorigin="anonymous"></script>
  <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/js/bootstrap.min.js"
    integrity="sha384-ChfqqxuZUCnJSK3+MXmPNIyE6ZbWh2IMqE241rYiqJxyMiZ6OW/JmZQ5stwEULTy"
    crossorigin="anonymous"></script>
</body>

</html>
